{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 循环神经网络的程序示例"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. RNN 架构"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 实现循环神经网络\n",
    "\n",
    "- 利用tensorflow实现的循环神经网络RNN（本程序使用了LSTM）来做语言模型，并输出其困惑度。\n",
    "- 语言模型主要是根据一段给定的文本来预测下一个词最有可能是什么。困惑度用于评价语言模型。困惑度越小，则模型的性能越好"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version :  2.3.0\n"
     ]
    }
   ],
   "source": [
    "import reader\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"tensorflow version : \", tf.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1 定义参数\n",
    "\n",
    "- data目录中应该预先存放本程序中要用到的语料，即PTB(Penn Treebank Dataset)数据集\n",
    "- PTB数据集是语言模型研究常用的数据集。其下载地址在 Tomas Mikolov的主页：\n",
    "- 下载地址 : https://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz。 (推荐使用迅雷下载)\n",
    "- 解压后，将其中的整个data文件夹拷贝到当前目录即可。数据集共有9998个单词，加上稀有词语的特殊符号<unk>和语句的结束标记，共有10000个单词。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 数据路径\n",
    "DATA_PATH = './data'\n",
    "\n",
    "# 超参数的设置\n",
    "# size of RNN hidden state, 每个单词词向量的维度\n",
    "HIDDEN_SIZE = 200\n",
    "# LSTM 层数\n",
    "NUM_LAYERS = 2\n",
    "# 词汇表中词的个数\n",
    "VOCAB_SIZE = 10000\n",
    "# 学习速率的超参数\n",
    "LEARNING_RATE = 1.0\n",
    "# 训练阶段每个数据批量设置为多少个样本, 本例中指若干个词构成的序列\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "# 训练阶段文本数据的截断长度, 也可以成为序列长度 seq_length\n",
    "TRAIN_NUM_STEP = 35\n",
    "# 训练的轮数\n",
    "NUM_EPOCH = 2\n",
    "# 节点不被 dropout 的概率\n",
    "KEEP_PROB = 0.5\n",
    "\n",
    "# 超参数, 避免梯度膨胀\n",
    "MAX_GRAD_NORM = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 定义多层循环神经网络"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        \"\"\"\n",
    "        初始化参数\n",
    "        :param is_training: 当前是否在训练阶段\n",
    "        :param batch_size: 批量的大小\n",
    "        :param num_steps: 数据的截断长度, 即序列长度\n",
    "        \"\"\"\n",
    "        # 定义输入层的数据维度为 batch_size * num_steps\n",
    "        self.input_data = tf.keras.Input(dtype=tf.int32, shape=[batch_size, num_steps])\n",
    "        # 定义输出层的数据维度为 batch_size * num_steps\n",
    "        self.targets = tf.keras.Input(dtype=tf.int32, shape=[batch_size, num_steps])\n",
    "\n",
    "        # 定义使用LSTM机构作为循环体的基本结构, 每个词向量的维度为 HIDDEN_SIZE\n",
    "        lstm_cell = tf.compat.v1.nn.rnn_cell.LSTMCell()\n",
    "\n",
    "        #如果是在训练阶段，则使用dropout.此时每个单元以（1-keep_prob）的概率不工作，目的是防止过拟合。\n",
    "        if is_training:\n",
    "            lstm_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=KEEP_PROB)\n",
    "\n",
    "        # 将多层RNN单元封装到一个单元Cell中, 层的个数NUM_LAYERS前面已经确定\n",
    "        cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell]*NUM_LAYERS)\n",
    "\n",
    "        # 使用zero_state函数初始化网络状态\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.int32)\n",
    "\n",
    "        # 将单词转换为词向量\n",
    "        # VOCAB_SIZE 词的总数, HIDDEN_SIZE 是每个单词的向量维度\n",
    "        # embedding 的参数为 : VOCAB_SIZE * HIDDEN_SIZE\n",
    "        # 则input的输入维度为 : batch_size * num_steps * HIDDEN_SIZE\n",
    "        embedding = tf.compat.v1.get_variable()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}