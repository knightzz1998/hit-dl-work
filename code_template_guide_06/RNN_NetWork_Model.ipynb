{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 循环神经网络的程序示例"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. RNN 架构"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 实现循环神经网络\n",
    "\n",
    "- 利用tensorflow实现的循环神经网络RNN（本程序使用了LSTM）来做语言模型，并输出其困惑度。\n",
    "- 语言模型主要是根据一段给定的文本来预测下一个词最有可能是什么。困惑度用于评价语言模型。困惑度越小，则模型的性能越好"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version :  2.3.0\n"
     ]
    }
   ],
   "source": [
    "import reader\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"tensorflow version : \", tf.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1 定义参数\n",
    "\n",
    "- data目录中应该预先存放本程序中要用到的语料，即PTB(Penn Treebank Dataset)数据集\n",
    "- PTB数据集是语言模型研究常用的数据集。其下载地址在 Tomas Mikolov的主页：\n",
    "- 下载地址 : https://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz。 (推荐使用迅雷下载)\n",
    "- 解压后，将其中的整个data文件夹拷贝到当前目录即可。数据集共有9998个单词，加上稀有词语的特殊符号<unk>和语句的结束标记，共有10000个单词。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 数据路径\n",
    "DATA_PATH = './data'\n",
    "\n",
    "# 超参数的设置\n",
    "# size of RNN hidden state, 每个单词词向量的维度\n",
    "HIDDEN_SIZE = 200\n",
    "# LSTM 层数\n",
    "NUM_LAYERS = 2\n",
    "# 词汇表中词的个数\n",
    "VOCAB_SIZE = 10000\n",
    "# 学习速率的超参数\n",
    "LEARNING_RATE = 1.0\n",
    "# 训练阶段每个数据批量设置为多少个样本, 本例中指若干个词构成的序列\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "# 训练阶段文本数据的截断长度, 也可以成为序列长度 seq_length\n",
    "TRAIN_NUM_STEP = 35\n",
    "# 训练的轮数\n",
    "NUM_EPOCH = 2\n",
    "# 节点不被 dropout 的概率\n",
    "KEEP_PROB = 0.5\n",
    "\n",
    "# 超参数, 避免梯度膨胀\n",
    "MAX_GRAD_NORM = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 定义多层循环神经网络\n",
    "\n",
    "- tf.compat.v1.get_variable\n",
    "    1. 该函数的作用是创建新的tensorflow变量\n",
    "    2. name :  变量名\n",
    "    3. shape : []\n",
    "\n",
    "- tf.nn.dropout()\n",
    "    1. 参考 [详解tf.nn.dropout](https://blog.csdn.net/yangfengling1023/article/details/82911306)\n",
    "    2. 该函数的作用是tensorflow里面为了防止或减轻过拟合而使用的函数，它一般用在全连接层\n",
    "    3. Dropout就是在不同的训练过程中随机扔掉一部分神经元。也就是让某个神经元的激活值以一定的概率p，让其停止工作，这次训练过程中不更新权值，也不参加神经网络的计算。但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了\n",
    "- tf.variable_scope()\n",
    "    1. 参考 [详解tf.variable_scope函数](https://docs.pythontab.com/tensorflow/how_tos/variable_scope/)\n",
    "    2. 用于定义创建变量（层）的操作的上下文管理器。此上下文管理器验证（可选）values是否来自同一图形，确保图形是默认的图形，并推送名称范围和变量范围"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        \"\"\"\n",
    "        初始化参数\n",
    "        :param is_training: 当前是否在训练阶段\n",
    "        :param batch_size: 批量的大小\n",
    "        :param num_steps: 数据的截断长度, 即序列长度\n",
    "        \"\"\"\n",
    "        # 定义输入层的数据维度为 batch_size * num_steps\n",
    "        self.input_data = tf.keras.Input(dtype=tf.int32, shape=[batch_size, num_steps])\n",
    "        # 定义输出层的数据维度为 batch_size * num_steps\n",
    "        self.targets = tf.keras.Input(dtype=tf.int32, shape=[batch_size, num_steps])\n",
    "\n",
    "        # 定义使用LSTM机构作为循环体的基本结构, 每个词向量的维度为 HIDDEN_SIZE\n",
    "        lstm_cell = tf.compat.v1.nn.rnn_cell.LSTMCell()\n",
    "\n",
    "        #如果是在训练阶段，则使用dropout.此时每个单元以（1-keep_prob）的概率不工作，目的是防止过拟合。\n",
    "        if is_training:\n",
    "            lstm_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=KEEP_PROB)\n",
    "\n",
    "        # 将多层RNN单元封装到一个单元Cell中, 层的个数NUM_LAYERS前面已经确定\n",
    "        cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell] * NUM_LAYERS)\n",
    "\n",
    "        # 使用zero_state函数初始化网络状态\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.int32)\n",
    "\n",
    "        # 将单词转换为词向量\n",
    "        # VOCAB_SIZE 词的总数, HIDDEN_SIZE 是每个单词的向量维度\n",
    "        # embedding 的参数为 : VOCAB_SIZE * HIDDEN_SIZE\n",
    "        # 则input的输入维度为 : batch_size * num_steps * HIDDEN_SIZE\n",
    "        embedding = tf.compat.v1.get_variable('embedding', [VOCAB_SIZE, HIDDEN_SIZE])\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        # 如果在训练阶段, 将进行 dropout\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, KEEP_PROB)\n",
    "\n",
    "        # 定义LSTM结构的输出列表\n",
    "        outputs = []\n",
    "        # 使用state存储LSTM的初始状态\n",
    "        state = self.initial_state\n",
    "        #TensorFlow提供了Variable Scope 机制，用于共享变量\n",
    "        with tf.compat.v1.variable_scope(\"RNN\"):\n",
    "            # 对于每个时间同步\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    # 重用变量\n",
    "                    tf.compat.v1.get_variable_scope().reuse_variables()\n",
    "                # 从输入数据inputs中获取当前时刻的输入并传入LSTM的单元\n",
    "                # 每次输出都是一个张量, shape = cell([20, 200]), 其中20是BATCH_SIZE, 200是词向量维度\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state)\n",
    "                # 将当前的单元的输出加入到输出的列表中\n",
    "                outputs.append(cell_output)\n",
    "\n",
    "        # 将输出的列表 outputs 利用tf.concat函数变成（batch,hidden_size*num_steps）的形状，然后再reshape成（batch*num_steps,hidden_size）的形状\n",
    "        # 即为 （20*35， 200）=（700, 200）\n",
    "        output = tf.resource(tf.concat(outputs, 1), [-1, HIDDEN_SIZE])\n",
    "\n",
    "        # 利用刚才LSTM的输出向量output乘以权重weight再加上偏置bias，得到最后的预测结果logits\n",
    "        # 获取权重值\n",
    "        weight = tf.compat.v1.get_variable('weight', [HIDDEN_SIZE, VOCAB_SIZE])  # weight 的形状为 [200,10000]\n",
    "        # 获取偏置值\n",
    "        bias = tf.compat.v1.get_variable('bias', [VOCAB_SIZE])\n",
    "        # 获取预测结果\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "\n",
    "        # 计算交叉熵损失\n",
    "        loss = tf.keras.losses.categorical_crossentropy(\n",
    "            [logits]  # 预测结果 shape=(700, 10000)\n",
    "            [tf.reshape(self.targets, [-1])]  # self.targets是正确的结果.这里对其shape进行了调整，变为shape=(700,)\n",
    "        )\n",
    "\n",
    "        # 计算每个 batch 的平均损失值\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        # 更新网络状态\n",
    "        self.final_state = state\n",
    "\n",
    "        # 如果当前是在训练阶段, 则进行下面的反向传播以更新梯度\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        # 返回需要训练的变量\n",
    "        trainable_variables = tf.compat.v1.trainable_variables()\n",
    "\n",
    "        # 通过clip_by_global_norm函数控制梯度的大小，避免梯度膨胀问题\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM\n",
    "        )\n",
    "\n",
    "        # 定义模型的优化方法\n",
    "        optimizer = tf.compat.v1.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "        # 应用梯度对 trainable_variables 更新\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 定义的函数run_epoch。使用刚才定义的模型model在数据data上运行train_op并返回perplexity值\n",
    "\n",
    "def run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3 定义主函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'flags'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_10796/2901467748.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtutorials\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mptb\u001B[0m \u001B[1;32mimport\u001B[0m  \u001B[0mreader\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\models\\tutorials\\rnn\\ptb\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;31m# import util\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtutorials\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mptb\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mreader\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 24\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtutorials\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mptb\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mutil\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\models\\tutorials\\rnn\\ptb\\util.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mrewriter_config_pb2\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m \u001B[0mFLAGS\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflags\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFLAGS\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'tensorflow' has no attribute 'flags'"
     ]
    }
   ],
   "source": [
    "from tensorflow.models.tutorials.rnn.ptb import  reader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}