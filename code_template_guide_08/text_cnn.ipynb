{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 基于 Embedding 与 CNN 进行文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=7, micro=11, releaselevel='final', serial=0)\n",
      "matplotlib 3.4.2\n",
      "numpy 1.18.5\n",
      "pandas 1.3.3\n",
      "sklearn 1.0\n",
      "tensorflow 2.3.0\n",
      "tensorflow.keras 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. 数据集读取与词表构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imdb 是电影评论数据集, 类别为积极与消极\n",
    "imdb = keras.datasets.imdb\n",
    "# 词表数量, 前 10000 个会被保存, 其他的会被当做特殊字符处理\n",
    "vocab_size = 10000\n",
    "# 词表的index从index_from 开始算, 因为要设置特殊字符! 0 ~ 3 是特殊字符\n",
    "index_from = 3\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size, index_from=index_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 1\n",
      "(25000,) (25000,)\n",
      "218 189\n"
     ]
    }
   ],
   "source": [
    "# 打印向量\n",
    "print(train_data[0], train_labels[0])\n",
    "print(train_data.shape, train_labels.shape)\n",
    "print(len(train_data[0]), len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 构建词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n"
     ]
    }
   ],
   "source": [
    "# 获取词表， word_index = (单词, 索引)\n",
    "word_index = imdb.get_word_index()\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "with 16\n",
      "one 28\n",
      "have 25\n",
      "i 10\n",
      "he 26\n",
      "as 14\n",
      "it 9\n",
      "is 6\n",
      "in 8\n",
      "but 18\n",
      "on 20\n",
      "of 4\n",
      "his 24\n",
      "all 29\n",
      "this 11\n",
      "not 21\n",
      "you 22\n",
      "a 3\n",
      "for 15\n",
      "be 27\n",
      "br 7\n",
      "the 1\n",
      "was 13\n",
      "and 2\n",
      "to 5\n",
      "film 19\n",
      "movie 17\n",
      "that 12\n",
      "are 23\n"
     ]
    }
   ],
   "source": [
    "print(type(word_index))\n",
    "# 打印前10个词表\n",
    "for word, index in word_index.items():\n",
    "    if int(index) < 30:\n",
    "        print(word, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 因为之前的设置 index_from = 3, 所以所有的索引都需要偏移3\n",
    "word_index = {word: (index + 3) for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 设置特殊字符\n",
    "# <PAD> padding 的填充字符\n",
    "# <START> 句子的开头的填充字符\n",
    "# <UNK> 找不到字符时, 返回UNK\n",
    "# <END> 句子的末尾的特殊字符\n",
    "\n",
    "word_index['<PAD>'] = 0\n",
    "word_index['<START>'] = 1\n",
    "word_index['<UNK>'] = 2\n",
    "word_index['<END>'] = 3\n",
    "reverse_word_index = dict([(index, word) for word, index in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用词表反向解析句子\n",
    "def decode_review(text_ids):\n",
    "    return \" \".join(\n",
    "        [reverse_word_index.get(word_id, \"<UNK>\") for word_id in text_ids]\n",
    "    )\n",
    "\n",
    "\n",
    "# 解析数字为真实文本\n",
    "decode_review(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. 数据padding\n",
    "\n",
    "- padding 就是对数据进行变长处理\n",
    "    1. 处理方式: 如果数据过长, 就截断数据, 如果数据过短就使用字符(0)填充\n",
    "    2. 合并\n",
    "- keras.preprocessing.sequence.pad_sequences\n",
    "    1. value=word_index['<PAD>'] , 使用词表中 'PAD' 对应的部分进行填充\n",
    "    2. padding='post' , value : post or pre , post 就是 把 padding 放到句子的后面, pre 就是 把padding 放到句子的前面\n",
    "    3. maxlen=max_length 句子的最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n"
     ]
    }
   ],
   "source": [
    "# 设置的单个句子的最长的长度\n",
    "\n",
    "seq_length = 500\n",
    "\n",
    "# 对训练数据进行padding\n",
    "\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    train_data,  # integer of list\n",
    "    value=word_index['<PAD>'],  # 使用词表中 'PAD' 对应的部分进行填充\n",
    "    padding='post',  # value : post or pre , post 就是 把 padding 放到句子的后面, pre 就是 把padding 放到句子的前面\n",
    "    maxlen=seq_length  # 句子的最大长度\n",
    ")\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    test_data,  # integer of list\n",
    "    value=word_index['<PAD>'],  # 使用词表中 'PAD' 对应的部分进行填充\n",
    "    padding='post',  # value : post or pre , post 就是 把 padding 放到句子的后面, pre 就是 把padding 放到句子的前面\n",
    "    maxlen=seq_length  # 句子的最大长度\n",
    ")\n",
    "\n",
    "# 打印样本\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 模型构建\n",
    "\n",
    "用一个例子解释textCNN的设计 :\n",
    "1. 这里的输入是一个有11个词的句子，每个词用6维词向量表示。因此输入序列的宽为11，输入通道数为6。\n",
    "2. 给定2个一维卷积核，核宽分别为2和4，输出通道数分别设为4和5。\n",
    "3. 因此，一维卷积计算后，4个输出通道的宽为11−2+1=10, 而其他5个通道的宽为11−4+1=8。\n",
    "4. 尽管每个通道的宽不同，我们依然可以对各个通道做时序最大池化，并将9个通道的池化输出连结成一个9维向量。最终，使用全连接将9维向量变换为2维输出，即正面情感和负面情感的预测\n",
    "\n",
    "- 参考文档\n",
    "    1. [TextCNN搭建情感分类](https://blog.csdn.net/qq_37541097/article/details/102520298)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 每个word都转换成长度为 16 的向量\n",
    "# 每个词用长度为 embedding_dim 的向量表示\n",
    "embedding_dim = 64\n",
    "# 每个单词的最大长度(文本  序列长度)\n",
    "seq_length = 500\n",
    "# 输入样本的宽为sentence_max_length, 高为1, 输入通道数为embedding_dim\n",
    "batch_size = 128\n",
    "num_cla = 2\n",
    "kernel_num = 64\n",
    "# 定义模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- func(x)(x) : [python中函数返回值是函数的函数的用法 func()()](https://blog.csdn.net/deliberate_cha/article/details/105869595)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def text_rnn(seq_length, vocab_size, embedding_dim, num_cla, kernel_num):\n",
    "    \"\"\"\n",
    "        seq_length: 输入的文字序列长度\n",
    "        vocab_size: 词汇库的大小\n",
    "        embedding_dim: 生成词向量的特征维度\n",
    "        num_cla: 分类类别\n",
    "        kernel_num:：卷积层的卷积核数\n",
    "    \"\"\"\n",
    "    # 定义输入层\n",
    "    input_layer = keras.layers.Input(shape=(seq_length))\n",
    "    # 嵌入层, 输入维度是 词汇库的大小, 输出是生成词向量的维度\n",
    "    # Embedding(...)(input_layer) 的作用是, Embedding函数的返回值是一个新的函数, 使用input_layer作为新的函数的参数\n",
    "    embedding_out = keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                           output_dim=embedding_dim,\n",
    "                                           input_length=seq_length)(input_layer)\n",
    "    # 分别使用长度为3,4,5的卷积核进行卷积操作, 然后执行池化处理\n",
    "    conv1 = keras.layers.Conv1D(filters=kernel_num,  # 卷积核的数量\n",
    "                                kernel_size=3,  # 卷积核的大小(窗口的大小)\n",
    "                                padding=\"valid\",  # valid 代表只进行有效的卷积, 即对边界数据不处理\n",
    "                                strides=1,  # 卷积的步长, 即窗口移动的步长\n",
    "                                activation=\"relu\")(embedding_out)\n",
    "    # 添加池化操作, 对简化特征\n",
    "    # pool_size : 最大池化窗口大小\n",
    "    mpd1 = keras.layers.MaxPool1D(pool_size=int(conv1.shape[1]))(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=kernel_num, kernel_size=4, padding=\"valid\", strides=1, activation=\"relu\")(mpd1)\n",
    "    mpd2 = keras.layers.MaxPool1D(pool_size=int(conv2.shape[1]))(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=kernel_num, kernel_size=5, padding=\"valid\", strides=1, activation=\"relu\")(mpd2)\n",
    "    mpd3 = keras.layers.MaxPool1D(pool_size=int(conv3.shape[1]))(conv3)\n",
    "\n",
    "    # 合并经过卷积核池化的向量\n",
    "    combine_cnn = keras.layers.Concatenate(axis=-1)([mpd1, mpd2, mpd3])\n",
    "    # 扁平化\n",
    "    flat_cnn = keras.layers.Flatten()(combine_cnn)\n",
    "\n",
    "    # 添加全连接层\n",
    "    dense_01 = keras.layers.Dense(128)(flat_cnn)\n",
    "    # 在全连接层1和2之间添加dropout减少训练过程中的过拟合，随机丢弃25%的结点值\n",
    "    dropout = keras.layers.Dropout(0.25)(dense_01)\n",
    "    # 为全连接层添加激活函数\n",
    "    dense_01_relu = keras.layers.ReLU()(dropout)\n",
    "    # 全连接层2（输出层）\n",
    "    predict_out = keras.layers.Dense(2, activation='softmax')(dense_01_relu)\n",
    "\n",
    "    # 指定模型的输入输出层\n",
    "    model = keras.models.Model(inputs=input_layer, ouputs=predict_out)\n",
    "    # 指定loss的计算方法，设置优化器，编译模型\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def text_cnn_example(seq_length, vocab_size, embedding_dim, num_cla, kernel_num):\n",
    "    \"\"\"\n",
    "    seq_length: 输入的文字序列长度\n",
    "    vocab_size: 词汇库的大小\n",
    "    embedding_dim: 生成词向量的特征维度\n",
    "    num_cla: 分类类别\n",
    "    kernel_num:：卷积层的卷积核数\n",
    "    \"\"\"\n",
    "\n",
    "    # 定义输入层\n",
    "    inputX = keras.layers.Input(shape=(seq_length,), dtype='int32')\n",
    "    # 嵌入层，将词汇的one-hot编码转为词向量\n",
    "    embOut = keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length)(inputX)\n",
    "\n",
    "    # 分别使用长度为3，4，5的词窗口去执行卷积, 接着进行最大池化处理\n",
    "    conv1 = keras.layers.Conv1D(kernel_num, 3, padding='valid', strides=1, activation='relu')(embOut)\n",
    "    maxp1 = keras.layers.MaxPool1D(pool_size=int(conv1.shape[1]))(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(kernel_num, 4, padding='valid', strides=1, activation='relu')(embOut)\n",
    "    maxp2 = keras.layers.MaxPool1D(pool_size=int(conv2.shape[1]))(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(kernel_num, 5, padding='valid', strides=1, activation='relu')(embOut)\n",
    "    maxp3 = keras.layers.MaxPool1D(pool_size=int(conv3.shape[1]))(conv3)\n",
    "\n",
    "    # 合并三个经过卷积和池化后的输出向量\n",
    "    combineCnn = keras.layers.Concatenate(axis=-1)([maxp1, maxp2, maxp3])\n",
    "    # 扁平化\n",
    "    flatCnn = keras.layers.Flatten()(combineCnn)\n",
    "\n",
    "    # 全连接层1，节点数为128\n",
    "    desen1 = keras.layers.Dense(128)(flatCnn)\n",
    "    # 在全连接层1和2之间添加dropout减少训练过程中的过拟合，随机丢弃25%的结点值\n",
    "    dropout = keras.layers.Dropout(0.25)(desen1)\n",
    "    # 为全连接层添加激活函数\n",
    "    densen1Relu = keras.layers.ReLU()(dropout)\n",
    "    # 全连接层2（输出层）\n",
    "    predictY = keras.layers.Dense(num_cla, activation='softmax')(densen1Relu)\n",
    "\n",
    "    # 指定模型的输入输出层\n",
    "    model = keras.models.Model(inputX, predictY)\n",
    "    # 指定loss的计算方法，设置优化器，编译模型\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 500, 64)      320000      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 498, 64)      12352       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 497, 64)      16448       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 496, 64)      20544       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling1D) (None, 1, 64)        0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling1D) (None, 1, 64)        0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling1D) (None, 1, 64)        0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1, 192)       0           max_pooling1d_18[0][0]           \n",
      "                                                                 max_pooling1d_19[0][0]           \n",
      "                                                                 max_pooling1d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 192)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          24704       flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 128)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 128)          0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            129         re_lu_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 394,177\n",
      "Trainable params: 394,177\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "# 每个单词的最大长度(文本  序列长度)\n",
    "seq_length = 500\n",
    "# 输入样本的宽为sentence_max_length, 高为1, 输入通道数为embedding_dim\n",
    "batch_size = 128\n",
    "num_cla = 1\n",
    "kernel_num = 64\n",
    "vocab_size = 5000\n",
    "model = text_cnn_example(seq_length, vocab_size, embedding_dim, num_cla, kernel_num)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "(25000, 500)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[0,8] = 8952 is not in [0, 5000)\n\t [[node functional_7/embedding_8/embedding_lookup (defined at C:\\Users\\11488\\AppData\\Local\\Temp/ipykernel_8872/1224145791.py:1) ]] [Op:__inference_train_function_2969]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_7/embedding_8/embedding_lookup:\n functional_7/embedding_8/embedding_lookup/2603 (defined at G:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\contextlib.py:112)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_8872/1224145791.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mhistory\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_labels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m30\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalidation_split\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    106\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m     \u001B[1;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1096\u001B[0m                 batch_size=batch_size):\n\u001B[0;32m   1097\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1098\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1099\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1100\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    778\u001B[0m       \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    779\u001B[0m         \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 780\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    781\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    782\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    838\u001B[0m         \u001B[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    839\u001B[0m         \u001B[1;31m# stateless function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 840\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    841\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    842\u001B[0m       \u001B[0mcanon_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcanon_kwds\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2827\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2828\u001B[0m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2829\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2830\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2831\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_filtered_call\u001B[1;34m(self, args, kwargs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1846\u001B[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001B[0;32m   1847\u001B[0m         \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1848\u001B[1;33m         cancellation_manager=cancellation_manager)\n\u001B[0m\u001B[0;32m   1849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1850\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_call_flat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1922\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1923\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[1;32m-> 1924\u001B[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[0;32m   1925\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1926\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    548\u001B[0m               \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 550\u001B[1;33m               ctx=ctx)\n\u001B[0m\u001B[0;32m    551\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    552\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[1;32mG:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[1;32m---> 60\u001B[1;33m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[0;32m     61\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m:  indices[0,8] = 8952 is not in [0, 5000)\n\t [[node functional_7/embedding_8/embedding_lookup (defined at C:\\Users\\11488\\AppData\\Local\\Temp/ipykernel_8872/1224145791.py:1) ]] [Op:__inference_train_function_2969]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_7/embedding_8/embedding_lookup:\n functional_7/embedding_8/embedding_lookup/2603 (defined at G:\\software\\anaconda3\\envs\\tf2-course-code\\lib\\contextlib.py:112)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(train_data, train_labels, epochs=30, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adam Optimizer是对SGD的扩展，可以代替经典的随机梯度下降法来更有效地更新网络权重\n",
    "    1. 计算效率高\n",
    "    2. 很少的内存需求\n",
    "    3. 梯度的对角线重缩放不变（这意味着亚当将梯度乘以仅带正因子的对角矩阵是不变的，以便更好地理解此堆栈交换）\n",
    "    4. 非常适合数据和/或参数较大的问题\n",
    "    5. 适用于非固定目标\n",
    "    6. 适用于非常嘈杂和/或稀疏梯度的问题\n",
    "    7. 超参数具有直观的解释，通常需要很少的调整（我们将在配置部分中对此进行详细介绍）\n",
    "\n",
    "- binary_crossentropy : 二进制交叉熵\n",
    "    1. 参考 : https://blog.csdn.net/qq_35599937/article/details/105608354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_cnn_model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=3, min_delta=1e-3)\n",
    "]\n",
    "history = text_cnn_model.fit(\n",
    "    train_data, train_labels, epochs=30, batch_size=batch_size,\n",
    "    validation_split=0.2,  # 设置验证集为 20%\n",
    "    #callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 绘制图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history, label, epochs, min_value, max_value):\n",
    "    data = {}\n",
    "    data[label] = history.history[label]\n",
    "    data[\"val_\" + label] = history.history[\"val_\" + label]\n",
    "    pd.DataFrame(data).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.axis([0, epochs, min_value, max_value])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# val_accuracy : 验证集的准确率\n",
    "# val_loss : 验证集的误差\n",
    "# 如下图可以看到, 验证集上的loss一段平稳过后直线上升, 说明出现了过拟合\n",
    "plot_learning_curves(history, 'accuracy', 30, 0, 1)\n",
    "plot_learning_curves(history, 'loss', 30, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 使用测试集进行验证\n",
    "text_cnn_model.evaluate(test_data, test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TexCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN配置参数\"\"\"\n",
    "\n",
    "    embedding_dim = 64  # 词向量维度\n",
    "    seq_length = 600  # 序列长度, 单个句子长度\n",
    "    num_classes = 2  # 类别数\n",
    "    num_filters = 256  # 卷积核数目\n",
    "    kernel_size = 5  # 卷积核尺寸\n",
    "    vocab_size = 5000  # 词汇表达小\n",
    "\n",
    "    hidden_dim = 128  # 全连接层神经元\n",
    "\n",
    "    dropout_keep_prob = 0.5  # dropout保留比例\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "\n",
    "    batch_size = 64  # 每批训练大小\n",
    "    num_epochs = 10  # 总迭代轮次\n",
    "\n",
    "    print_per_batch = 100  # 每多少轮输出一次结果\n",
    "    save_per_batch = 10  # 每多少轮存入tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}